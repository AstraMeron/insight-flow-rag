{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6256f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories found: ['Credit Card' 'Savings Account' 'Money Transfer' 'Personal Loan']\n",
      "\n",
      "--- Stratified Sample Distribution ---\n",
      "Category\n",
      "Credit Card        7036\n",
      "Savings Account    7036\n",
      "Personal Loan       811\n",
      "Money Transfer      116\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total Sample Size: 14999\n",
      "\n",
      "Sample saved to data/processed/sampled_complaints.csv\n"
     ]
    }
   ],
   "source": [
    "# --- TASK 2: STEP 1 - Stratified Sampling ---\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the cleaned dataset from Task 1\n",
    "df = pd.read_csv('../data/filtered_complaints.csv')\n",
    "\n",
    "def create_stratified_sample(df, target_total=15000):\n",
    "    # Identify our categories\n",
    "    categories = df['Category'].unique()\n",
    "    print(f\"Categories found: {categories}\")\n",
    "    \n",
    "    # We will aim for a balanced distribution where possible\n",
    "    # but since some categories are very small, we take all of them.\n",
    "    \n",
    "    small_cats = ['Money Transfer', 'Personal Loan']\n",
    "    df_small = df[df['Category'].isin(small_cats)]\n",
    "    \n",
    "    # Calculate how many slots are left for the big categories\n",
    "    remaining_slots = target_total - len(df_small)\n",
    "    large_cats = [c for c in categories if c not in small_cats]\n",
    "    slots_per_large_cat = remaining_slots // len(large_cats)\n",
    "    \n",
    "    # Sample the large categories\n",
    "    sampled_list = [df_small]\n",
    "    for cat in large_cats:\n",
    "        cat_df = df[df['Category'] == cat]\n",
    "        sampled_list.append(cat_df.sample(n=slots_per_large_cat, random_state=42))\n",
    "    \n",
    "    # Combine and shuffle\n",
    "    df_final = pd.concat(sampled_list).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    return df_final\n",
    "\n",
    "# Execute sampling\n",
    "df_sample = create_stratified_sample(df)\n",
    "\n",
    "# Show results for verification\n",
    "print(\"\\n--- Stratified Sample Distribution ---\")\n",
    "print(df_sample['Category'].value_counts())\n",
    "print(f\"\\nTotal Sample Size: {len(df_sample)}\")\n",
    "\n",
    "# Create the processed directory if it doesn't exist\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "# Save the sample\n",
    "df_sample.to_csv('../data/processed/sampled_complaints.csv', index=False)\n",
    "print(\"\\nSample saved to data/processed/sampled_complaints.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38157d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original records: 14999\n",
      "Total chunks created: 16734\n",
      "\n",
      "--- Chunk Preview ---\n",
      "        category                                         text_chunk\n",
      "0    Credit Card  my information was heisted and this accounts a...\n",
      "1    Credit Card                            i signed up for an card\n",
      "2    Credit Card  there were a multitude of fraudulent charges m...\n",
      "3  Personal Loan  on statementno date on when maild payment due ...\n",
      "4    Credit Card  see the attached documents i want the bureau t...\n"
     ]
    }
   ],
   "source": [
    "# --- TASK 2: STEP 2 - Text Chunking ---\n",
    "\n",
    "# Updated Import for current LangChain versions\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Initialize the splitter\n",
    "# chunk_size: ~100 words (600 characters)\n",
    "# chunk_overlap: to keep context across splits\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# 2. Function to process our dataframe into chunks\n",
    "def chunk_data(df):\n",
    "    chunked_records = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # Split the cleaned narrative\n",
    "        chunks = text_splitter.split_text(row['cleaned_narrative'])\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunked_records.append({\n",
    "                'chunk_id': f\"{idx}_{i}\",       # Unique ID for each chunk\n",
    "                'complaint_id': idx,            # Trace back to original row index\n",
    "                'category': row['Category'],    # Metadata: Product category\n",
    "                'text_chunk': chunk             # The actual text to embed\n",
    "            })\n",
    "            \n",
    "    return pd.DataFrame(chunked_records)\n",
    "\n",
    "# 3. Create the chunked dataframe\n",
    "print(f\"Original records: {len(df_sample)}\")\n",
    "df_chunks = chunk_data(df_sample)\n",
    "\n",
    "print(f\"Total chunks created: {len(df_chunks)}\")\n",
    "print(\"\\n--- Chunk Preview ---\")\n",
    "print(df_chunks[['category', 'text_chunk']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "343894d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and loading the 'all-MiniLM-L6-v2' model...\n",
      "\n",
      "--- Model Loaded Successfully ---\n",
      "Model Name: all-MiniLM-L6-v2\n",
      "Vector Dimensions: 384\n",
      "Sample Vector (first 5 values): [-0.00969655  0.03044972 -0.10841177 -0.01133379  0.01064291]\n"
     ]
    }
   ],
   "source": [
    "# --- TASK 2: STEP 3 - Loading the Embedding Model ---\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the model (this will download it on the first run, ~22MB)\n",
    "print(\"Downloading and loading the 'all-MiniLM-L6-v2' model...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Test with a single chunk to see the vector\n",
    "test_text = df_chunks['text_chunk'].iloc[0]\n",
    "test_vector = model.encode(test_text)\n",
    "\n",
    "print(\"\\n--- Model Loaded Successfully ---\")\n",
    "print(f\"Model Name: all-MiniLM-L6-v2\")\n",
    "print(f\"Vector Dimensions: {len(test_vector)}\")\n",
    "print(f\"Sample Vector (first 5 values): {test_vector[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cee5d5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 16734 chunks in batches of 5000...\n",
      "Encoding and adding batch 1...\n",
      "Encoding and adding batch 2...\n",
      "Encoding and adding batch 3...\n",
      "Encoding and adding batch 4...\n",
      "\n",
      "Successfully stored 16734 vectors in 'vector_store/'.\n"
     ]
    }
   ],
   "source": [
    "# --- TASK 2: STEP 4 - Vector Store Indexing (Batch Fixed) ---\n",
    "\n",
    "import chromadb\n",
    "\n",
    "# 1. Initialize the Persistent Client\n",
    "client = chromadb.PersistentClient(path=\"../vector_store\")\n",
    "collection = client.get_or_create_collection(name=\"bank_complaints\")\n",
    "\n",
    "# 2. Define the batch size based on your error message\n",
    "BATCH_SIZE = 5000 \n",
    "total_chunks = len(df_chunks)\n",
    "\n",
    "print(f\"Adding {total_chunks} chunks in batches of {BATCH_SIZE}...\")\n",
    "\n",
    "# 3. Loop through the data in batches\n",
    "for i in range(0, total_chunks, BATCH_SIZE):\n",
    "    # Slice the data for the current batch\n",
    "    batch_df = df_chunks.iloc[i : i + BATCH_SIZE]\n",
    "    \n",
    "    # Prepare batch data\n",
    "    batch_ids = batch_df['chunk_id'].astype(str).tolist()\n",
    "    batch_docs = batch_df['text_chunk'].tolist()\n",
    "    batch_metadatas = batch_df[['complaint_id', 'category']].to_dict(orient='records')\n",
    "    \n",
    "    # Generate embeddings for the current batch\n",
    "    print(f\"Encoding and adding batch {i//BATCH_SIZE + 1}...\")\n",
    "    batch_embeddings = model.encode(batch_docs, show_progress_bar=False).tolist()\n",
    "    \n",
    "    # Add the batch to the collection\n",
    "    collection.add(\n",
    "        ids=batch_ids,\n",
    "        embeddings=batch_embeddings,\n",
    "        metadatas=batch_metadatas,\n",
    "        documents=batch_docs\n",
    "    )\n",
    "\n",
    "print(f\"\\nSuccessfully stored {collection.count()} vectors in 'vector_store/'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd87d09d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
